{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Convolutional Neural Network\n",
    "\n",
    "## Background\n",
    "A Convolutional Neural Network (CNN or ConvNet) is a type of deep learning model most commonly used for analyzing visual imagery. As a system inspired by the mammel's visual cortex, it's the technology behind tasks like image recognition, object detection, and facial recognition.\n",
    "\n",
    "Computer vision is the primary and most common use of ConvNet.  \n",
    "\n",
    "- Social media platforms use it to automatically suggest tags for people (facial recognition) or objects in your photos.\n",
    "- Cashier-less stores (like Amazon Go) use it to track what items you pick up.\n",
    "- Self-Driving cars use it to detect and react to people, other cars, traffic lights, and road signs.\n",
    "\n",
    "While most famous for images, CNNs can also be applied to sequential data :\n",
    "\n",
    "- Sentiment Analysis: Determining if a product review or tweet is positive, negative, or neutral.\n",
    "- Text Classification: Automatically sorting documents or articles by topic.\n",
    "- Speech Recognition: Used in virtual assistants (like Siri or Google Assistant) to help process and understand the sounds in your voice.\n",
    "- Finance: Analyzing time-series data (like stock charts) to detect patterns or fraudulent activity.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Review data loading and pre-processing using PyTorch. \n",
    "2. Practice construction of Convolutional Neural Network\n",
    "3. Practice model training with PyTorch.\n",
    "\n",
    "<font color=582c83>\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. (30%) Exercise 1: ConvNet construction\n",
    "2. (20%) Exercise 2: Training function\n",
    "3. (10%) Exercise 3: Validationion function\n",
    "4. (20%) Exercise 4: Model optimization\n",
    "5. (20%) Exercise 5: New images classification\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "We will train a deep learning model using the ImageNette dataset.\n",
    "Imagenette is a subset of 10 easily classified classes from Imagenet (tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute).\n",
    "\n",
    "- The raw images in ImageNette dataset have various resolutions, and we will resize them to 160x120x3.\n",
    "- Two dataloaders will be created for easily batching in training and validation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms.v2 import Compose, ToImage, RGB, Resize, ToDtype\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Construct transform pipeline for input features\n",
    "transform_pipeline = Compose([\n",
    "    ToImage(),\n",
    "    RGB(),\n",
    "    Resize((160, 120)),\n",
    "    ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "# Download datasets\n",
    "dataset_train = datasets.Imagenette(\n",
    "    root=\"data\",\n",
    "    split=\"train\",\n",
    "    size=\"160px\",\n",
    "    download=True,\n",
    "    transform=transform_pipeline,\n",
    ")\n",
    "labels = [c[0] for c in dataset_train.classes]\n",
    "\n",
    "dataset_val = datasets.Imagenette(\n",
    "   root=\"data\",\n",
    "    split=\"val\",\n",
    "    size=\"160px\",\n",
    "    download=True,\n",
    "    transform=transform_pipeline,\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Number of training samples: {len(dataset_train)}\")\n",
    "print(f\"Number of validation samples: {len(dataset_val)}\")\n",
    "print(f\"Features shape: {dataset_train[0][0].shape}\")\n",
    "print(f\"Categories: {labels}\")\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 64\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Visualize data samples\n",
    "sample_batch_train = next(iter(dataloader_train))\n",
    "fig, axs = plt.subplots(5, 5, figsize=(10, 10))\n",
    "for i in range(25):\n",
    "    sample_img = sample_batch_train[0][i].permute(1, 2, 0).numpy()  # reconstruct image to (H, W, C)\n",
    "    sample_cls = sample_batch_train[1][i].item()\n",
    "    sample_lbl = labels[sample_cls]\n",
    "    axs[i//5, i%5].imshow(sample_img)\n",
    "    axs[i//5, i%5].set_title(sample_lbl)\n",
    "    axs[i//5, i%5].axis(\"off\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convolutional Neural Network (ConvNet) Construction\n",
    "\n",
    "Design your ConvNet's architecture to classify images (shape: `(3, 160, 120)`) into 10 categories.\n",
    "The output feature matrix dimension can be computed using the following equation.\n",
    "\n",
    "$$\n",
    "W_{out} = \\frac{W_{in} - K + 2P}{S} + 1\n",
    "$$\n",
    "Given the dimension of the input matrix (either horizontal or vertical), $W_{in}$ is the length of the corresponding input matrix's dimension, $K$ is the kernel/filter length, $P$ is the padding length, $S$ is the stride of convolution. $W_{out}$ is the length of the corresponding output matrix's dimension.\n",
    "\n",
    "For example, if the input matrix is with shape `(5, 8)`, a `(3, 3)` kernel applied on 1 pixel padded input with stride of 2 will output a `(3, 4)` matrix.\n",
    "\n",
    "> Check this [post](https://www.geeksforgeeks.org/machine-learning/cnn-introduction-to-padding/) for more details.\n",
    "\n",
    "### <font color=#582c83> (30%) Exercise 1: ConvNet construction </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "### START CODE HERE ###\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "### END CODE HERE ###\n",
    "        return y\n",
    "\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device}.\")\n",
    "model = ConvNet().to(device)  # use GPU if available\n",
    "# print(model)`(5, 8)`\n",
    "from torchinfo import summary\n",
    "summary(model, input_size=(batch_size, 3, 160, 120))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ConvNet Training\n",
    "\n",
    "### <font color=#582c83> (20%) Exercise 2: Training function </font>\n",
    "Model optimization in one epoch using all samples in the dataset which are organized in batches.\n",
    "\n",
    "Repeat until all batches are used:\n",
    "1. Get a batch of features and lables.\n",
    "2. Make prediction.\n",
    "3. Calculate loss\n",
    "4. Compute gradients of loss with back-propagation.\n",
    "5. Update model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    sum_losses, sum_correct_counts = 0, 0\n",
    "    for batch_index, (X, y) in enumerate(dataloader):\n",
    "        ### START CODE HERE ### (~ 6 lines)\n",
    "        X, y = None, None\n",
    "        # Compute prediction error\n",
    "        batch_preds = None\n",
    "        batch_loss = None\n",
    "        # Backpropagation\n",
    "        None\n",
    "        None\n",
    "        None\n",
    "        ### END CODE HERE ###\n",
    "        # Stats\n",
    "        sum_losses += batch_loss.item()\n",
    "        sum_correct_counts += (batch_preds.argmax(1) == y).type(torch.float).sum().item()\n",
    "        # Log\n",
    "        if batch_index % 50 == 0:  # print every 50 batches\n",
    "            print(f\"Training batch: [{(batch_index+1)*len(y):>5d}/{size:>5d}] loss: {batch_loss.item():>7f}\")\n",
    "    # Summarize epoch metrics\n",
    "    epoch_loss = sum_losses / len(dataloader)\n",
    "    epoch_accuracy = sum_correct_counts / size\n",
    "    print(f\"Training: \\n Accuracy: {(100*epoch_accuracy):>0.1f}%, Avg loss: {epoch_loss:>8f} \\n\")\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#582c83> (10%) Exercise 3: Validationion function </font>\n",
    "Classification accuracy is an important metric. \n",
    "Calculate accuracy using validation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    sum_losses, sum_correct_counts = 0, 0\n",
    "    with torch.no_grad():   \n",
    "        for X, y in dataloader:\n",
    "            ### START CODE HERE ### (~ 3 lines)\n",
    "            X, y = None, None\n",
    "            pred = None\n",
    "            sum_losses += loss_fn(pred, y).item()\n",
    "            sum_correct_counts += None\n",
    "            ### END CODE HERE ###\n",
    "    # Summarize epoch metrics\n",
    "    epoch_loss = sum_losses / len(dataloader)\n",
    "    epoch_accuracy = sum_correct_counts / size\n",
    "    print(f\"Validation: \\n Accuracy: {(100*epoch_accuracy):>0.1f}%, Avg loss: {epoch_loss:>8f} \\n\")\n",
    "    return epoch_loss, epoch_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#582c83> (20%) Exercise 4: Model optimization </font>\n",
    "Use appropriate hyper-parameters and functions to optimize the ConvNet model.\n",
    "\n",
    "> - `SGD` is not the only optimization [algorithm](https://docs.pytorch.org/docs/stable/optim.html#algorithms)\n",
    "> - Pick a reasonable [loss function](https://docs.pytorch.org/docs/stable/nn.html#loss-functions).\n",
    "\n",
    "<font color=red> Validation accuracy is expected to beyond **70%** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "            \n",
    "### START CODE HERE ### (~ 5 lines)\n",
    "# Initialize model\n",
    "model = ConvNet().to(device)\n",
    "# Hyperparameters\n",
    "loss_fn = None\n",
    "optimizer = None\n",
    "num_epochs = None\n",
    "# Metrics storage\n",
    "losses_train, losses_val = [], []\n",
    "accuracies_train, accuracies_val = [], []\n",
    "# Training loop\n",
    "for t in range(num_epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    ep_loss_train, ep_acc_train = None\n",
    "    ep_loss_val, ep_acc_val = None\n",
    "    losses_train.append(ep_loss_train)\n",
    "    accuracies_train.append(ep_acc_train)\n",
    "    losses_val.append(ep_loss_val)\n",
    "    accuracies_val.append(ep_acc_val)\n",
    "print(\"Done!\")\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Visualize training metrics\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(num_epochs), losses_train, range(num_epochs), losses_val)\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(num_epochs), accuracies_train, range(num_epochs), accuracies_val)\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test Model\n",
    "There are 10 pictures prepared in the [test_images](./test_images/) directory (1 testing image in each category).\n",
    "Please classify these images using the trained model.\n",
    "\n",
    "### <font color=#582c83> (20%) Exercise 5: New images classification </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torchvision.io import decode_image, ImageReadMode\n",
    "\n",
    "# Glob test files\n",
    "test_dir = Path.cwd() / \"test_images\"  # locate dataset directory from this repo in the whole system\n",
    "test_files = list(test_dir.glob(\"**/*.jpg\"))\n",
    "# print(test_files)\n",
    "\n",
    "# Predict test images with trained model\n",
    "model.eval()\n",
    "with torch.no_grad():  # ensure model will not be updated\n",
    "    for file in test_files:\n",
    "        img_raw = decode_image(file, mode=ImageReadMode.RGB)  # decode image file to pytorch tensor\n",
    "        ### START CODE HERE ### (~ 3 lines)\n",
    "        img_resize = None  # resize image to comply with model input size\n",
    "        image_test = None  # rescale and add batch dimension\n",
    "        pred_test = None  # predict with trained model\n",
    "        ### END CODE HERE ###\n",
    "        print(f\"Predicted {file.name} class: {labels[pred_test.argmax().item()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congrats on finishing this assignment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "convnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
